apiVersion: batch/v1
kind: Job
metadata:
  name: experiment1 # Your training job name
spec:
  backoffLimit: 0   # add this line so --> If your pod failed, the training job will stop.
  template:
    spec:
      restartPolicy: Never
      imagePullSecrets:
        - name: gitlab-docker-secret
      volumes:
        - name: shared-memory # This is to resolve the dataloader OOM issue.
          emptyDir:
            medium: Memory
        - name: dataset-volume
          persistentVolumeClaim:
            claimName: brain-segmentation-pvc       # <------- your PVC name    kubectl get pvc
      containers:
        - name: experiment1 # It is recommended to name your container according to the project
          image: docker.aiml.team/products/aiml/docker-example/pytorch-2.3.1-cuda12.1-cudnn8-devel:latest # You can reuse this image for different projects if their python dependencies are the same.
          imagePullPolicy: Always
          stdin: true
          tty: true
          command: ["/bin/bash", "-c"]
          args:            
            - |
              cd 
              pip install --upgrade pip
              git clone "https://github.com/victorcaquilpan/Brain-Tumors-Segmentation"
              cd Brain-Tumors-Segmentation
              pip install -r requirements.txt
              python train.py dataset.dataset_folder=/data/data-brain/ training.max_epochs=2 training.batch_size=1 training.val_every=1 training.learning_rate=1e-4 model.architecture=nn_former
          resources:
            limits:
              nvidia.com/gpu: 1              # <------- If you don't need any of GPUs, Please comment-out this line.
          volumeMounts:
            - name: dataset-volume
              mountPath: /data
            - name: shared-memory
              mountPath: /dev/shm
          env:
            - name: GITLAB_TOKEN
              valueFrom:
                secretKeyRef:
                    name: gitlab-token  # kubectl get secrets
                    key:  access-token